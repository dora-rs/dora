# Dora MLX-LM Node

## Overview

The `dora-mlx-lm` node integrates the [`mlx-lm`](https://github.com/ml-explore/mlx-lm) library to run large language models (LLMs) optimized for Apple Silicon (M1, M2, M3, and later) on macOS. It processes text prompts as input and generates text responses using a model such as `mlx-community/SmolLM-135M-Instruct-4bit`. The node is designed for use within a [Dora-rs-cli](https://github.com/dora-rs/dora) pipeline, supporting features like activation words, conversation history, and performance metadata.`.

## Installation

To use the `dora-mlx-lm` node, install the required dependencies:

```bash
pip install dora-rs-cli mlx-lm
```

## Usage

1. **Add the node to your Dora pipeline**:

   Include the `dora-mlx-lm` node in your pipeline YAML file. Below is an example configuration:

   ```yaml
   nodes:
     - id: mlx_lm
       build: pip install mlx-lm
       path: dora-mlx-lm/main.py
       inputs:
         prompt: dora/input
       outputs:
         - text
       env:
         MODEL_PATH: mlx-community/SmolLM-135M-Instruct-4bit
         SYSTEM_PROMPT: "You are a helpful assistant optimized for Apple M-series chips."
         MAX_TOKENS: "100"
         TEMPERATURE: "0.7"
         CONTEXT_SIZE: "2048"
         ACTIVATION_WORDS: "hey assistant"
   ```

   ### Environment Variables
   - `MODEL_PATH`: Path or Hugging Face ID of the model (default: `mlx-community/SmolLM-135M-Instruct-4bit`).
   - `SYSTEM_PROMPT`: Optional system prompt to define the model's behavior (default: empty).
   - `MAX_TOKENS`: Maximum number of tokens to generate (default: 100).
   - `TEMPERATURE`: Sampling temperature for generation (default: 0.7).
   - `CONTEXT_SIZE`: Maximum context length for conversation history (default: 2048).
   - `ACTIVATION_WORDS`: Space-separated list of words to trigger the node (default: empty, processes all inputs).

2. **Run the pipeline**:

   Build and execute your pipeline using the Dora CLI:

   ```bash
   dora build your_pipeline.yml --uv
   dora run your_pipeline.yml --uv
   ```
## Inputs

- **prompt**: A text string to be processed by the LLM (e.g., "Write a short story about a robot"). The node validates that the input is a non-empty `pyarrow.Array` containing a string.

## Outputs

- **text**: The text response generated by the LLM, sent as a `pyarrow.Array`. The output includes metadata such as:
  - `processing_time`: Time taken to generate the response (in seconds).
  - `model`: The model used (e.g., `mlx-community/SmolLM-135M-Instruct-4bit`).
  - `optimized_for`: Indicates optimization for Apple's M-series chips.

## Features

- **Apple Silicon Optimization**: Leverages the MLX framework for efficient inference on M1, M2, M3, and later chips, with automatic GPU and Neural Engine acceleration.
- **Conversation History**: Maintains a conversation history with a configurable system prompt, truncated based on `CONTEXT_SIZE`.
- **Activation Words**: Optionally processes inputs only when they contain specified activation words.
- **Robust Error Handling**: Validates inputs and logs errors for reliable pipeline integration.
- **Metadata**: Provides performance metrics and configuration details in output metadata.

### Using mlx-lm in Dora Node Hub
- **Platform**: macOS 13.5+ (ARM-native Python required)
- Note: This node is only supported on macOS and skips execution on Linux/Windows.

## Notes

- The node uses `mlx-lm`, which is optimized for Apple Silicon. Parameters like `N_GPU_LAYERS` or `N_THREADS` (common in other frameworks like `llama_cpp`) are not applicable, as MLX manages resource allocation internally.
- For large models, use quantized versions (e.g., 4-bit) to optimize memory usage and performance.
- The conversation history is truncated to respect the `CONTEXT_SIZE` limit, ensuring compatibility with the model's context length.

## License

This node is licensed under the [MIT License](https://opensource.org/licenses/MIT), consistent with the `mlx-lm` library.