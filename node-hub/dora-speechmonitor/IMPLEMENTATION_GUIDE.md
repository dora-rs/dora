# dora-speechmonitor Implementation Guide

## Table of Contents
1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Installation](#installation)
4. [Usage Guide](#usage-guide)
5. [Configuration](#configuration)
6. [API Reference](#api-reference)
7. [Implementation Details](#implementation-details)
8. [Examples](#examples)
9. [Troubleshooting](#troubleshooting)
10. [Performance](#performance)

## Overview

The `dora-speechmonitor` is a real-time speech detection node for Dora, inspired by VoiceDialogue's production-grade SpeechStateMonitor implementation. It provides low-latency speech detection with state tracking, smart buffering, and comprehensive event outputs.

### Key Features
- **Real-time Processing**: < 100ms latency from speech to detection
- **State Machine**: Tracks speech phases (SILENCE â†’ SPEAKING â†’ TRAILING_SILENCE)
- **Smart Buffering**: Pre-speech and post-speech padding for complete capture
- **Multiple Outputs**: 5 different event streams for comprehensive monitoring
- **Task Management**: Unique IDs for tracking speech segments through pipeline
- **Production Ready**: Based on battle-tested VoiceDialogue implementation

### Comparison with Alternatives

| Feature | dora-speechmonitor | dora-vad | Raw Silero VAD |
|---------|-------------------|----------|----------------|
| Latency | 100ms | 1000ms+ | 30ms |
| State Tracking | Yes | No | No |
| Pre-speech Buffer | Yes | No | No |
| Events | 5 types | 2 types | None |
| Max Buffer | 10s dynamic | 10s fixed | N/A |
| Task Management | Yes | No | No |

## Architecture

### Component Diagram
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio Input    â”‚
â”‚  (Microphone)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 16kHz, float32
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Speech Monitor â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Silero VAD    â”‚
â”‚ â€¢ State Machine â”‚
â”‚ â€¢ Buffer Mgmt   â”‚
â”‚ â€¢ Task Tracking â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼         â–¼        â–¼          â–¼             â–¼
speech_   speech_   is_      audio_      speech_
started    ended   speaking  segment   probability
```

### State Machine
```mermaid
stateDiagram-v2
    [*] --> SILENCE
    SILENCE --> SPEAKING: VAD detects speech
    SPEAKING --> TRAILING_SILENCE: VAD detects silence
    TRAILING_SILENCE --> SPEAKING: Speech resumes
    TRAILING_SILENCE --> SILENCE: Silence > threshold
    SILENCE --> [*]
```

### Data Flow
1. **Audio chunks** arrive (100ms, 1600 samples at 16kHz)
2. **VAD processing** in 32ms windows (512 samples)
3. **State updates** based on detection
4. **Buffer management** with pre/post padding
5. **Event emission** on state changes
6. **Segment output** when speech ends

## Installation

### Prerequisites
```bash
# Required packages
pip install dora-rs numpy pyarrow torch silero-vad sounddevice
```

### Install from node-hub
```bash
cd node-hub/dora-speechmonitor
pip install -e .
```

### Verify Installation
```python
from dora_speechmonitor import main
print("Speech monitor installed successfully")
```

## Usage Guide

### Basic Setup

1. **Create a dataflow configuration** (`speech_detection.yml`):
```yaml
nodes:
  - id: speech-monitor
    build: pip install -e ../../node-hub/dora-speechmonitor
    path: dora-speechmonitor
    inputs:
      audio: YOUR_AUDIO_SOURCE/audio
    outputs:
      - speech_started
      - speech_ended
      - is_speaking
      - audio_segment
      - speech_probability
```

2. **Start the dataflow**:
```bash
dora start speech_detection.yml --attach
```

### With Dynamic Microphone

```yaml
nodes:
  # Dynamic microphone
  - id: microphone
    path: dynamic
    outputs:
      - audio
  
  # Speech monitor
  - id: speech-monitor
    build: pip install -e ../../node-hub/dora-speechmonitor
    path: dora-speechmonitor
    inputs:
      audio: microphone/audio
    outputs:
      - speech_started
      - speech_ended
      - is_speaking
      - audio_segment
      - speech_probability
```

Run microphone separately:
```python
from dora import Node
import sounddevice as sd
import pyarrow as pa

node = Node(node_id="microphone")
# ... audio capture and sending logic
```

### Processing Speech Events

```python
from dora import Node

node = Node()
for event in node:
    if event["type"] == "INPUT":
        if event["id"] == "speech_started":
            timestamp = event["value"][0].as_py()
            print(f"Speech started at {timestamp}")
            
        elif event["id"] == "audio_segment":
            audio = event["value"].to_numpy()
            metadata = event["metadata"]
            duration_ms = metadata["duration_ms"]
            print(f"Received {duration_ms}ms of speech")
```

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MIN_AUDIO_AMPLITUDE` | 0.01 | Minimum amplitude to consider as speech |
| `ACTIVE_FRAME_THRESHOLD_MS` | 100 | Time before marking speech as active |
| `USER_SILENCE_THRESHOLD_MS` | 1000 | Time to consider user done speaking |
| `SILENCE_THRESHOLD_MS` | 300 | Time to end speech segment |
| `AUDIO_FRAMES_THRESHOLD_MS` | 10000 | Maximum segment length (10s) |
| `VAD_THRESHOLD` | 0.7 | Silero VAD confidence threshold |
| `VAD_ENABLED` | true | Enable/disable VAD processing |
| `SAMPLE_RATE` | 16000 | Audio sample rate (Hz) |
| `QUEUE_TIMEOUT` | 0.1 | Queue timeout in seconds |

### Configuration Examples

#### Sensitive Detection (Quick Response)
```yaml
env:
  VAD_THRESHOLD: 0.5              # More sensitive
  SILENCE_THRESHOLD_MS: 200       # Faster cutoff
  MIN_AUDIO_AMPLITUDE: 0.005      # Lower noise floor
```

#### Robust Detection (Noisy Environment)
```yaml
env:
  VAD_THRESHOLD: 0.8              # Less sensitive
  SILENCE_THRESHOLD_MS: 500       # Longer wait
  MIN_AUDIO_AMPLITUDE: 0.02       # Higher noise floor
```

#### Long Form Speech (Presentations)
```yaml
env:
  AUDIO_FRAMES_THRESHOLD_MS: 30000  # 30 seconds max
  USER_SILENCE_THRESHOLD_MS: 2000   # 2 second user silence
```

## API Reference

### Inputs

#### `audio`
- **Type**: PyArrow array of float32
- **Format**: Mono audio chunks
- **Sample Rate**: 16000 Hz (configurable)
- **Chunk Size**: Flexible, typically 100ms (1600 samples)
- **Metadata**: `{"sample_rate": 16000}`

### Outputs

#### `speech_started`
- **Type**: PyArrow array with timestamp
- **Value**: Unix timestamp when speech began
- **Metadata**:
  ```python
  {
    "task_id": "unique-task-id",
    "segment": 1  # Segment counter
  }
  ```

#### `speech_ended`
- **Type**: PyArrow array with timestamp
- **Value**: Unix timestamp when speech ended
- **Metadata**:
  ```python
  {
    "task_id": "unique-task-id",
    "segment": 1,
    "duration": 2.5  # Duration in seconds
  }
  ```

#### `is_speaking`
- **Type**: PyArrow array with boolean
- **Value**: Current speaking state
- **Frequency**: Every audio chunk
- **Metadata**: `{"timestamp": unix_timestamp}`

#### `audio_segment`
- **Type**: PyArrow array of float32
- **Value**: Complete speech audio segment
- **Metadata**:
  ```python
  {
    "task_id": "unique-task-id",
    "session_id": "session-id",
    "answer_id": "answer-id",
    "duration_ms": 2500,
    "is_over_threshold": false,
    "segment": 1,
    "forced": false  # True if cut due to max length
  }
  ```

#### `speech_probability`
- **Type**: PyArrow array with float
- **Value**: VAD confidence (0.0 to 1.0)
- **Frequency**: Every audio chunk
- **Metadata**: `{"timestamp": unix_timestamp}`

## Implementation Details

### VAD Processing

The node uses Silero VAD with sliding window processing:

```python
# Window-based processing
window_size = 512  # 32ms at 16kHz
for i in range(0, len(audio), window_size):
    window = audio[i:i + window_size]
    probability = vad_model(window, 16000)
    if probability > threshold:
        is_speech = True
```

### Buffer Management

Three types of buffers are maintained:

1. **Pre-speech Buffer** (200ms)
   - Rolling buffer during silence
   - Captures speech onset
   - Prepended to speech segments

2. **Speech Buffer** (dynamic)
   - Accumulates during active speech
   - Includes trailing silence
   - Sent when speech ends

3. **Segment Buffer** (max 10s)
   - Prevents memory overflow
   - Forces segment end at limit
   - Resets for next segment

### State Tracking

```python
class SpeechState(Enum):
    SILENCE = "silence"
    SPEAKING = "speaking"  
    TRAILING_SILENCE = "trailing_silence"
```

State transitions trigger events:
- `SILENCE â†’ SPEAKING`: Send `speech_started`
- `TRAILING_SILENCE â†’ SILENCE`: Send `speech_ended` + `audio_segment`

### Task Management

Each speech segment gets unique identifiers:
- **task_id**: Unique per speech segment
- **session_id**: Groups related segments
- **answer_id**: For response tracking

## Examples

### Complete Speech Detection Pipeline

```python
#!/usr/bin/env python3
"""Complete speech detection with transcription"""

from dora import Node
import numpy as np

def process_speech():
    node = Node()
    
    for event in node:
        if event["type"] == "INPUT":
            event_id = event["id"]
            
            if event_id == "speech_started":
                print("ðŸŽ¤ User started speaking")
                
            elif event_id == "speech_ended":
                duration = event["metadata"]["duration"]
                print(f"ðŸ”‡ User stopped speaking ({duration:.1f}s)")
                
            elif event_id == "audio_segment":
                audio = event["value"].to_numpy()
                task_id = event["metadata"]["task_id"]
                
                # Send to transcription service
                transcription = transcribe_audio(audio)
                print(f"ðŸ“ Transcription: {transcription}")
                
            elif event_id == "speech_probability":
                prob = event["value"][0].as_py()
                if prob > 0.9:
                    print("ðŸ”Š Strong speech detected")
```

### Real-time Voice Assistant

```python
def voice_assistant():
    node = Node()
    is_processing = False
    
    for event in node:
        if event["type"] == "INPUT":
            if event["id"] == "is_speaking":
                is_speaking = event["value"][0].as_py()
                
                if is_speaking and is_processing:
                    # User interrupting - stop current response
                    stop_assistant_response()
                    is_processing = False
                    
            elif event["id"] == "audio_segment":
                if not is_processing:
                    audio = event["value"].to_numpy()
                    is_processing = True
                    
                    # Process user query
                    text = transcribe(audio)
                    response = generate_response(text)
                    play_response(response)
                    
                    is_processing = False
```

### Speech Analytics

```python
def analyze_speech_patterns():
    node = Node()
    speech_times = []
    silence_times = []
    last_event_time = None
    
    for event in node:
        if event["type"] == "INPUT":
            current_time = time.time()
            
            if event["id"] == "speech_started":
                if last_event_time:
                    silence_times.append(current_time - last_event_time)
                last_event_time = current_time
                
            elif event["id"] == "speech_ended":
                if last_event_time:
                    speech_times.append(current_time - last_event_time)
                last_event_time = current_time
                
                # Calculate metrics
                avg_speech = np.mean(speech_times) if speech_times else 0
                avg_silence = np.mean(silence_times) if silence_times else 0
                
                print(f"Average speech duration: {avg_speech:.2f}s")
                print(f"Average pause duration: {avg_silence:.2f}s")
```

## Troubleshooting

### Common Issues

#### No Speech Detected
- **Check microphone permissions**
- **Verify audio levels**: Use test script to check peak > 0.01
- **Lower VAD_THRESHOLD**: Try 0.5 or 0.4
- **Reduce MIN_AUDIO_AMPLITUDE**: Try 0.005

#### Speech Cut Off Too Early
- **Increase SILENCE_THRESHOLD_MS**: Try 500ms or 700ms
- **Check audio levels**: Might be dropping below threshold

#### Too Many False Positives
- **Increase VAD_THRESHOLD**: Try 0.8 or 0.9
- **Increase MIN_AUDIO_AMPLITUDE**: Try 0.02
- **Add noise filtering** before speech monitor

#### High Latency
- **Check CPU usage**: VAD needs ~5% of single core
- **Reduce audio chunk size**: Try 50ms chunks
- **Disable debug output**: Remove print statements

### Debug Mode

Enable detailed logging:
```python
# In your node configuration
env:
  DEBUG_MODE: true  # Add debug output
```

### Test Your Setup

```bash
# Test microphone
python test_mic_levels.py

# Test VAD sensitivity
python test_vad_threshold.py

# Full pipeline test
dora start test_pipeline.yml --attach
```

## Performance

### Benchmarks

| Metric | Value | Notes |
|--------|-------|-------|
| Latency | 80-120ms | From speech to event |
| CPU Usage | 3-5% | Single core at 16kHz |
| Memory | 50MB | Model + buffers |
| Accuracy | >95% | Clean audio |
| Max Throughput | 100 streams | Per CPU core |

### Optimization Tips

1. **CPU Optimization**
   - Use lower sample rate (8kHz) if possible
   - Increase chunk size to 200ms for batch processing
   - Disable unused outputs

2. **Memory Optimization**
   - Reduce max segment length
   - Clear buffers after processing
   - Use smaller pre-speech buffer

3. **Network Optimization**
   - Batch events when possible
   - Compress audio segments
   - Use local processing

## Advanced Topics

### Custom VAD Models

Replace Silero VAD with custom model:
```python
class CustomVAD:
    def is_voice_active(self, audio, sample_rate):
        # Your VAD logic
        return is_speech, confidence
```

### Multi-language Support

Configure for different languages:
```yaml
env:
  VAD_THRESHOLD: 0.6  # Adjust per language
  # Future: LANGUAGE: "zh"  # Chinese, etc.
```

### Integration with ASR

Chain with speech recognition:
```yaml
nodes:
  - id: speech-monitor
    path: dora-speechmonitor
    outputs:
      - audio_segment
      
  - id: whisper
    path: dora-whisper
    inputs:
      audio: speech-monitor/audio_segment
```

## Contributing

Contributions welcome! Key areas:
- Additional VAD models
- Language-specific optimizations
- Performance improvements
- Better state machine
- WebRTC integration

## License

MIT License - See LICENSE file

## Credits

Inspired by VoiceDialogue's production SpeechStateMonitor implementation.
Uses Silero VAD for speech detection.