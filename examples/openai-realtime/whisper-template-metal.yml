nodes:
  - id: NODE_ID
    build: pip install -e ../../node-hub/dora-openai-websocket
    path: dynamic
    inputs:
      audio: tts/audio
      transcript: stt/text
      text: llm/text
      speech_started: dora-vad/timestamp_start
      speech_stopped: dora-vad/timestamp_end
    outputs:
      - audio
      - response.create
      - function_call_output

  - id: dora-vad
    build: pip install -e ../../node-hub/dora-vad
    path: dora-vad
    inputs:
      audio:
        source: NODE_ID/audio
        queue_size: 1000000
    outputs:
      - audio
      - timestamp_start
      - timestamp_end
    env:
      MIN_SPEECH_DURATION_MS: 1000
      MIN_SILENCE_DURATION_MS: 1000
      THRESHOLD: 0.5

  - id: stt
    build: pip install -e ../../node-hub/dora-distil-whisper
    path: dora-distil-whisper
    inputs:
      audio: dora-vad/audio
      text_noise: llm/text
    outputs:
      - text
      - word
      - speech_started
    env:
      TARGET_LANGUAGE: english

  - id: llm
    build: pip install -e ../../node-hub/dora-qwen
    path: dora-qwen
    inputs:
      text: stt/text
      response.create: NODE_ID/response.create
      text_tool_response: NODE_ID/function_call_output
    outputs:
      - text
    env:
      MODEL_NAME_OR_PATH: LLM_ID
      MODEL_FILE_PATTERN: "*[qQ]6_[kK].[gG][gG][uU][fF]"
      MAX_TOKENS: 30_000
      CONTEXT_SIZE: 30_000
      TOOLS_JSON: >
        TOOLS_ID

  - id: tts
    build: pip install -e ../../node-hub/dora-kokoro-tts
    path: dora-kokoro-tts
    inputs:
      text: llm/text
    outputs:
      - audio
